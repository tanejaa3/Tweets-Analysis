{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns',None)\n",
    "pd.set_option('display.max_rows',None)\n",
    "\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 10,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Dl and NLP libraries\n",
    "import nltk\n",
    "import keras\n",
    "import re\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding,LSTM,Dense,Flatten,GlobalAveragePooling1D\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object iniatilize\n",
    "eng_words = set(stopwords.words('english'))\n",
    "p_stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing train and test data\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data concat\n",
    "data = pd.concat([train_data,test_data],0,sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Having a look at data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables creation\n",
    "data_id = data['id']\n",
    "Y = data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data function\n",
    "def clean_text(corpus):\n",
    "    clean_corpus = []\n",
    "    for i in range(len(corpus)):\n",
    "        sentence = re.sub('[^a-zA-Z]',' ',corpus[i])\n",
    "        sentence = sentence.lower()\n",
    "        sentence = sentence.split()\n",
    "        sentence = [p_stemmer.stem(word) for word in sentence if word not in eng_words]\n",
    "        sentence = ' '.join(sentence)\n",
    "        clean_corpus.append(sentence)\n",
    "    return clean_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning train data\n",
    "corpus = clean_text(data['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word cloud\n",
    "sent_of_string = ' '.join(corpus)\n",
    "plt.imshow(WordCloud().generate(sent_of_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Naive Bayes\n",
    "\n",
    "# Count Vectorizer\n",
    "cv = CountVectorizer()\n",
    "corpus_cv = cv.fit_transform(corpus).toarray()\n",
    "\n",
    "# Train | Validation_data\n",
    "train_x = corpus_cv[:7613]\n",
    "train_y = Y.iloc[:7613]\n",
    "validation = corpus_cv[7613:]\n",
    "\n",
    "# Dividing train data into train and test\n",
    "xtrain,xtest,ytrain,ytest = train_test_split(train_x,train_y,test_size=0.2,random_state=0)\n",
    "\n",
    "# Model buildings - Naive Bayes\n",
    "nb = MultinomialNB()\n",
    "nb.fit(xtrain,ytrain)\n",
    "pred_nb = nb.predict(xtest)\n",
    "\n",
    "# Desired results\n",
    "print(accuracy_score(ytest,pred_nb))\n",
    "print(f1_score(ytest,pred_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Random Forest\n",
    "\n",
    "# Count Vectorizer\n",
    "cv = CountVectorizer()\n",
    "corpus_cv = cv.fit_transform(corpus).toarray()\n",
    "\n",
    "# Train | Validation_data\n",
    "train_x = corpus_cv[:7613]\n",
    "train_y = Y.iloc[:7613]\n",
    "validation = corpus_cv[7613:]\n",
    "\n",
    "# Dividing train data into train and test\n",
    "xtrain,xtest,ytrain,ytest = train_test_split(train_x,train_y,test_size=0.2,random_state=0)\n",
    "\n",
    "# Model buildings - Xg Boost\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "rf.fit(xtrain,ytrain)\n",
    "pred_rf = rf.predict(xtest)\n",
    "\n",
    "# Desired results\n",
    "print(accuracy_score(ytest,pred_rf))\n",
    "print(f1_score(ytest,pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tf-Idf Vectorizer\n",
    "tf = TfidfVectorizer()\n",
    "corpus_tf = tf.fit_transform(corpus).toarray()\n",
    "\n",
    "# Train | Validation_data\n",
    "train_x = corpus_tf[:7613]\n",
    "train_y = Y.iloc[:7613]\n",
    "#validation = corpus_cv[7613:]\n",
    "\n",
    "# Dividing train data into train and test\n",
    "xtrain,xtest,ytrain,ytest = train_test_split(train_x,train_y,test_size=0.2,random_state=0)\n",
    "\n",
    "# Model buildings\n",
    "nb = MultinomialNB()\n",
    "nb.fit(xtrain,ytrain)\n",
    "pred_nb = nb.predict(xtest)\n",
    "\n",
    "# Desired results\n",
    "print(accuracy_score(ytest,pred_nb))\n",
    "print(f1_score(ytest,pred_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Embedding and LSTM\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "sequence = tokenizer.texts_to_sequences(corpus)\n",
    "padded_sequence = pad_sequences(sequence)\n",
    "\n",
    "train_x = padded_sequence[:7613]\n",
    "\n",
    "# Dividing train data into train and test\n",
    "xtrain,xtest,ytrain,ytest = train_test_split(train_x,train_y,test_size=0.2,random_state=0)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(50000,32,input_length=27))\n",
    "model.add(LSTM(30,activation='relu',dropout=0.1))\n",
    "model.add(Dense(units=1,activation='sigmoid'))\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(xtrain,ytrain.map(int),batch_size=32,epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lstm = model.predict(xtest)\n",
    "pred_lstm = pred_lstm>0.5\n",
    "\n",
    "# Desired results\n",
    "print(accuracy_score(ytest,pred_lstm))\n",
    "print(f1_score(ytest,pred_lstm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final data submission\n",
    "predictions = nb.predict(validation).astype(int)\n",
    "out_of_sample_ids = data_id[7613:]\n",
    "submission = pd.DataFrame({'id':out_of_sample_ids,'target':predictions})\n",
    "submission.to_csv('Final_Submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
